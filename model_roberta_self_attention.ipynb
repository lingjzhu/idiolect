{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3\n", "# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Created on Sun Sep 27 17:06:21 2020"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "import os<br>\n", "import pandas as pd<br>\n", "import torch<br>\n", "import transformers<br>\n", "import numpy as np<br>\n", "import argparse<br>\n", "from torch import nn<br>\n", "import torch.nn.functional as F<br>\n", "#from transformers.modeling_roberta import RobertaPreTrainedModel<br>\n", "from transformers import RobertaConfig, RobertaModel, AdamW,RobertaTokenizerFast<br>\n", "from sklearn.utils import shuffle<br>\n", "from tqdm import tqdm<br>\n", "from sklearn.metrics import f1_score,roc_auc_score<br>\n", "#from entmax import sparsemax<br>\n", "# preprocess the texts<br>\n", "def loads(path):<br>\n", "    data = pd.read_csv(path,sep=\"\\t\\t\",header=None)<br>\n", "    <br>\n", "    labels = data[4].tolist()<br>\n", "    texts = [(i,j) for i,j in zip(data[2].tolist(),data[3].tolist())]<br>\n", "    users = data[0].tolist()<br>\n", "    products = data[1].tolist()<br>\n", "    return texts, labels, users, products<br>\n", "#text_tr, text_te, label_tr, label_te = train_test_split(texts,labels,test_size=0.1)<br>\n", "def preprocess(texts):<br>\n", "    <br>\n", "    pair_a = []<br>\n", "    pair_b = []<br>\n", "    for i in texts:<br>\n", "        a, b = i<br>\n", "        pair_a.append(a)<br>\n", "        pair_b.append(b)<br>\n", "    <br>\n", "    return pair_a, pair_b<br>\n", "def tokenize(textA,textB,seq_len=102):<br>\n", "    <br>\n", "    length = len(textA)<br>\n", "    input_ids_a = torch.ones(length,seq_len).long()<br>\n", "    input_ids_b = torch.ones(length,seq_len).long()<br>\n", "    attention_mask_a = torch.zeros(length,seq_len).long()<br>\n", "    attention_mask_b = torch.zeros(length,seq_len).long()<br>\n", "    <br>\n", "    for k, a, b in tqdm(zip(range(length),textA, textB)):<br>\n", "        seqA = tokenizer.encode_plus(a,add_special_tokens=True, max_length=seq_len,truncation=True,return_tensors=\"pt\")<br>\n", "        seqB = tokenizer.encode_plus(b,add_special_tokens=True, max_length=seq_len,truncation=True,return_tensors=\"pt\")<br>\n", "        if args.permute_words==True:<br>\n", "            seqA['input_ids'] = seqA['input_ids'][:,torch.randperm(seqA['input_ids'].size(1))]<br>\n", "            seqB['input_ids'] = seqB['input_ids'][:,torch.randperm(seqB['input_ids'].size(1))]<br>\n", "        input_ids_a[k,:seqA['input_ids'].shape[1]] = seqA['input_ids']<br>\n", "        input_ids_b[k,:seqB['input_ids'].shape[1]] = seqB['input_ids']<br>\n", "        attention_mask_a[k,:seqA['attention_mask'].shape[1]] = seqA['attention_mask']<br>\n", "        attention_mask_b[k,:seqB['attention_mask'].shape[1]] = seqB['attention_mask']<br>\n", "    return input_ids_a,input_ids_b,attention_mask_a,attention_mask_b<br>\n", "def batchify(text_a,text_b,attn_a,attn_b,labels,size=16,shuffling=False):<br>\n", "    length = len(text_a)<br>\n", "    <br>\n", "    if shuffling == True:<br>\n", "        text_a,text_b,attn_a,attn_b,labels = shuffle(text_a,text_b,attn_a,attn_b,labels)<br>\n", "        <br>\n", "    for i in range(0,length,size):<br>\n", "        sampleA = text_a[i:min(i+size,length)]<br>\n", "        sampleB = text_b[i:min(i+size,length)]<br>\n", "        sample_attn_a = attn_a[i:min(i+size,length)]<br>\n", "        sample_attn_b = attn_b[i:min(i+size,length)]<br>\n", "        y = labels[i:min(i+size,length)]<br>\n", "        yield sampleA, sampleB, sample_attn_a, sample_attn_b, torch.tensor(y).long()<br>\n", "def masking(text,mlm_prob=0.1,mask=50264):<br>\n", "    <br>\n", "    indices_replaced = torch.bernoulli(torch.full(text.shape, mlm_prob)).bool()<br>\n", "    text[indices_replaced] = mask<br>\n", "    return text<br>\n", "    <br>\n", "def margin_loss(diff, target, tau_low=0.2, tau_high=0.8):<br>\n", "    <br>\n", "    within = target*torch.max(diff-tau_low,torch.tensor(0.0).to(device))**2<br>\n", "    without = (1-target)*torch.max(tau_high-diff,torch.tensor(0.0).to(device))**2<br>\n", "    <br>\n", "    return torch.mean(within)+torch.mean(without)<br>\n", "def proxy_anchor_loss(cosine,target,delta=0.5,alpha=10):<br>\n", "  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    Implementation of the loss in:\n", "        https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Proxy_Anchor_Loss_for_Deep_Metric_Learning_CVPR_2020_paper.pdf\n", "    '''\n", "    positive = cosine[target==1]\n", "    negative = cosine[target!=1]\n", "    pos_loss = torch.mean(F.softplus(torch.logsumexp(-alpha*(positive.unsqueeze(1) - delta),dim=1)))\n", "    neg_loss = torch.mean(F.softplus(torch.logsumexp(alpha*(negative.unsqueeze(1) + delta),dim=1)))\n", "    \n", "    \n", "    if torch.isnan(pos_loss):\n", "        return neg_loss\n", "    elif torch.isnan(neg_loss):\n", "        return pos_loss\n", "    else:\n", "        return pos_loss+neg_loss\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def modified_anchor_loss(cosine,target,delta_s=0.8,delta_d=0.1,alpha=10):\n", "    positive = cosine[target==1]\n", "    negative = cosine[target!=1]\n", "    pos_loss = torch.mean(F.softplus(torch.logsumexp(-alpha*(positive.unsqueeze(1) - delta_s),dim=1)))\n", "    neg_loss = torch.mean(F.softplus(torch.logsumexp(alpha*(negative.unsqueeze(1) - delta_d),dim=1)))\n", "    \n", "    \n", "    if torch.isnan(pos_loss):\n", "        return neg_loss\n", "    elif torch.isnan(neg_loss):\n", "        return pos_loss\n", "    else:\n", "        return pos_loss+neg_loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def margin_anchor_loss(cosine,target,delta_s=0.8,delta_d=0.2,alpha=10):\n", "    positive = cosine[target==1]\n", "    negative = cosine[target!=1]\n", "    \n", "    pos_loss = torch.mean(F.softplus(torch.logsumexp(alpha*torch.max(delta_s - positive.unsqueeze(1),torch.tensor(0.0).to(device)),dim=1)))\n", "    neg_loss = torch.mean(F.softplus(torch.logsumexp(alpha*torch.max(negative.unsqueeze(1) - delta_d,torch.tensor(0.0).to(device)),dim=1)))\n", "    \n", "    if torch.isnan(pos_loss):\n", "        return neg_loss\n", "    elif torch.isnan(neg_loss):\n", "        return pos_loss\n", "    else:\n", "        return pos_loss+neg_loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def spherical_regularizer(embeddings,eta=0.5):\n", "    '''\n", "    Implementation of Eq (17) in \n", "    https://proceedings.neurips.cc/paper/2020/file/d9812f756d0df06c7381945d2e2c7d4b-Paper.pdf\n", "    '''    \n", "    \n", "    norms = torch.norm(embeddings,dim=1)\n", "    norms = norms - torch.mean(norms)\n", "    return eta*torch.mean(norms)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AttentionPooling(nn.Module):\n", "    \"\"\"\n", "    Implementation of SelfAttentionPooling\n", "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n", "    https://arxiv.org/pdf/2008.01077v1.pdf\n", "    \"\"\"\n", "    def __init__(self, input_dim):\n", "        super(AttentionPooling, self).__init__()\n", "        self.W = nn.Linear(input_dim, 1)\n", "        self.softmax = nn.functional.softmax\n", "    def forward(self, batch_rep, att_mask=None):\n", "        \"\"\"\n", "            N: batch size, T: sequence length, H: Hidden dimension\n", "            input:\n", "                batch_rep : size (N, T, H)\n", "            attention_weight:\n", "                att_w : size (N, T, 1)\n", "            return:\n", "                utter_rep: size (N, H)\n", "        \"\"\"\n", "        att_logits = self.W(batch_rep).squeeze(-1)\n", "        if att_mask is not None:\n", "            att_logits = att_mask + att_logits\n", "        att_w = self.softmax(att_logits, dim=-1).unsqueeze(-1)\n", "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n", "        return utter_rep"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DNNSelfAttention(nn.Module):\n", "    def __init__(\n", "        self,\n", "        hidden_dim,\n", "        **kwargs\n", "    ):\n", "        super(DNNSelfAttention, self).__init__()\n", "        self.pooling = AttentionPooling(hidden_dim)\n", "        self.out_layer = nn.Sequential(\n", "            nn.Linear(hidden_dim, hidden_dim),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_dim, hidden_dim),\n", "        )\n", "    def forward(self, features, att_mask):\n", "        out = self.pooling(features, att_mask).squeeze(-1)\n", "        predicted = self.out_layer(out)\n", "        return predicted"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SRoberta(nn.Module):\n", "    \n", "    def __init__(self,model_name):\n", "        super().__init__()\n", "        \n", "        self.roberta = RobertaModel.from_pretrained(model_name,return_dict=True)\n", "        self.pooler = DNNSelfAttention(768)\n", "        \n", "    def forward(self,input_ids,att_mask=None):\n", "        out = self.roberta(input_ids,att_mask)\n", "        out = out.last_hidden_state\n", "        out = self.pooler(out,att_mask)\n", "        return out"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SRoberta_vanilla(nn.Module):\n", "    def __init__(self,model_name):\n", "        super().__init__()\n", "        self.roberta = RobertaModel.from_pretrained(model_name,return_dict=True)\n", "        \n", "    def forward(self, inut_ids,att_mask):\n", "        out = self.roberta(input_ids,att_mask)\n", "        out = out.last_hidden_state\n", "        return out\n", " "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(args,model,train_a,train_b,tr_attn_a, tr_attn_b, label_tr):\n", "    model.train()\n", "    for i, (pair_a, pair_b, attna, attnb, y) in tqdm(enumerate(batchify(train_a,train_b,tr_attn_a, tr_attn_b, label_tr,size=args.training_bsz,shuffling=True))):\n", "        \n", "        if args.mask_prob!=0:\n", "            pair_a = masking(pair_a,mlm_prob=args.mask_prob)\n", "            pair_b = masking(pair_b,mlm_prob=args.mask_prob)\n", "        \n", "        hiddenA = model(input_ids=pair_a.to(device),att_mask=attna.to(device))\n", "        hiddenB = model(input_ids=pair_b.to(device),att_mask=attnb.to(device))\n", "        \n", "        \n", "        if args.spherical:\n", "            reg = 0.5*spherical_regularizer(hiddenA,eta=args.eta) + 0.5*spherical_regularizer(hiddenB,eta=args.eta)\n", "        \n", "        if args.loss == 'anchor':\n", "            hiddenA = F.normalize(hiddenA,dim=1)\n", "            hiddenB = F.normalize(hiddenB,dim=1)\n", "            cosine = torch.sum(hiddenA*hiddenB,dim=1)\n", "            loss = 0.1*loss_fn(cosine,y.to(device).float(),delta=args.delta,alpha=args.alpha)\n", "            \n", "        elif args.loss == 'modified_anchor' or args.loss == 'margin_anchor':\n", "            hiddenA = F.normalize(hiddenA,dim=1)\n", "            hiddenB = F.normalize(hiddenB,dim=1)\n", "            cosine = torch.sum(hiddenA*hiddenB,dim=1)\n", "            loss = 0.1*loss_fn(cosine,y.to(device).float(),delta_s=args.tau_high,delta_d=args.tau_low,alpha=args.alpha)\n", "            \n", "        elif args.loss == 'margin':\n", "            if args.distance == 'cosine':\n", "                hiddenA = F.normalize(hiddenA,dim=1)\n", "                hiddenB = F.normalize(hiddenB,dim=1)\n", "                cosine = 1.0-torch.sum(hiddenA*hiddenB,dim=1)\n", "                loss = loss_fn(cosine,y.to(device).float(),tau_low=args.tau_low,tau_high=args.tau_high)\n", "            elif args.distance == 'euclidean':\n", "                cosine = torch.norm(hiddenA - hiddenB,dim=1)\n", "                loss = loss_fn(cosine,y.to(device).float(),tau_low=args.tau_low,tau_high=args.tau_high)\n", "            \n", "        if args.spherical:\n", "            loss = loss + reg\n", "        \n", "        loss.backward()\n", "        if i%args.grad_acc==0:\n", "            optimizer.step()\n", "            optimizer.zero_grad()\n", "            print(\"The loss is: %s\"%loss.item())\n", "        \n", "        \n", "                "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(args,test_a,test_b,te_attn_a, te_attn_b, label_te):\n", "    #evaluate\n", "    logits = []\n", "    targets = []\n", "    model.eval()\n", "    for i, (pair_a, pair_b, attna, attnb, y) in tqdm(enumerate(batchify(test_a,test_b,te_attn_a, te_attn_b, label_te,size=args.test_bsz))):\n", "        with torch.no_grad():\n", "            hiddenA = model(pair_a.to(device),att_mask=attna.to(device))\n", "            hiddenB = model(pair_b.to(device),att_mask=attnb.to(device))\n", "            \n", "            if args.distance == 'cosine':\n", "                hiddenA = F.normalize(hiddenA,dim=1)\n", "                hiddenB = F.normalize(hiddenB,dim=1)\n", "                logit = 1.0-torch.sum(hiddenA*hiddenB,dim=1)\n", "            elif args.distance == 'euclidean':\n", "                logit = torch.norm(hiddenA - hiddenB,dim=1)\n", "                \n", "            logits += list(logit.detach().cpu().numpy())\n", "            targets += list(y.detach().cpu().numpy())\n", "    return logits,targets\n", "        \n", "        \n", "def compute_metric(logits,targets,threshold):\n", "    scores = [1 if i<threshold else 0 for i in logits]        \n", "    accuracy = (np.array(targets)==np.array(scores))\n", "    accuracy = sum(accuracy)/len(accuracy)\n", "    f1 = f1_score(targets,scores)\n", "    auc = roc_auc_score(targets,1-np.array(logits))\n", "    print(accuracy)        \n", "    print(f1_score(targets,scores)) \n", "    print(auc)\n", "    return round(accuracy,3),round(f1,3),round(auc,3)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    \n", "    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"--model\",default='roberta-base',type=str)\n", "    parser.add_argument(\"--attpool\",default=True,type=bool)\n", "    parser.add_argument(\"--loss\", default='anchor',type=str)\n", "    parser.add_argument(\"--training_data\",default=None,type=str)\n", "    parser.add_argument(\"--develop_data\",default=None,type=str)\n", "    parser.add_argument(\"--test_data\",default=None,type=str)\n", "    parser.add_argument(\"--train\",action='store_true')\n", "    parser.add_argument(\"--test\",action='store_true')\n", "    # parser.add_argument(\"--save_model\",default='/gpfs/accounts/lingjzhu_root/lingjzhu1/lingjzhu/authorship_models',type=str)\n", "    parser.add_argument(\"--save_model\",default='./authorship_models',type=str)\n", "    parser.add_argument(\"--load_model\",default=None,type=str)\n", "    # parser.add_argument(\"--save_cache\",default=None,type=str)\n", "    parser.add_argument(\"--save_cache\", default='./authorship_models', type=str)\n", "    parser.add_argument(\"--load_cache\",default=None,type=str)\n", "    parser.add_argument(\"--distance\",default='cosine',type=str)\n", "    parser.add_argument(\"--tau_low\",default=0.2,type=float)\n", "    parser.add_argument(\"--tau_high\",default=0.8,type=float)\n", "    parser.add_argument(\"--epochs\",default=3,type=int)\n", "    parser.add_argument(\"--grad_acc\",default=8,type=int)\n", "    parser.add_argument(\"--mask_prob\",default=0.1,type=float)\n", "    parser.add_argument(\"--lr\",default=1e-5,type=float)\n", "    parser.add_argument('--training_bsz',default=32,type=int)\n", "    parser.add_argument('--test_bsz',default=80,type=int)\n", "    parser.add_argument('--save_test_data',action='store_true')\n", "    parser.add_argument('--permute_words',action='store_true')\n", "    parser.add_argument('--alpha',default=10,type=float)\n", "    parser.add_argument('--delta',default=0.5,type=float)\n", "    parser.add_argument('--spherical',action='store_true')\n", "    parser.add_argument('--eta', default=0.5, type=float)\n", "    parser.add_argument('--prefix',default=None,type=str)\n", "    args = parser.parse_args()\n\n", "    # device = \"cuda\"\n", "    device = \"cpu\"\n", "    #initiate the tokenizer\n", "    tokenizer = RobertaTokenizerFast.from_pretrained(args.model)\n", "    #initiate the models\n", "    if args.load_model:\n", "        model = torch.load(args.load_model)\n", "    else:\n", "        if args.attpool == True:\n", "            model = SRoberta('roberta-base').to(device)\n", "        else:\n", "            model = SRoberta_vanilla('roberta-base').to(device)\n", "    optimizer = AdamW(model.parameters(),lr=args.lr)\n", "    \n", "    if args.loss == 'margin':\n", "        loss_fn = margin_loss\n", "        threshold = 0.5*args.tau_low+0.5*args.tau_high\n", "    elif args.loss == 'anchor':\n", "        loss_fn = proxy_anchor_loss\n", "        threshold = args.delta\n", "    elif args.loss == 'modified_anchor':\n", "        loss_fn = modified_anchor_loss\n", "        threshold = 0.5*args.tau_low+0.5*args.tau_high\n", "    elif args.loss == 'margin_anchor':\n", "        loss_fn = margin_anchor_loss\n", "        threshold = 0.5*args.tau_low+0.5*args.tau_high        \n", "        \n", "    \n", "    \n", "    \n", "    if args.loss == 'margin':\n", "        model_name = \"roberta-%s-%s-%s-mask-%s\"%(args.distance,args.tau_low,args.tau_high,args.mask_prob)\n", "    elif args.loss == 'anchor':\n", "        model_name = \"roberta-%s-%s-mask-%s-delta-%s-alpha-%s\"%(args.distance,args.loss,args.mask_prob,args.delta,args.alpha)\n", "    elif args.loss == 'modified_anchor':\n", "        model_name = \"roberta-%s-%s-mask-%s-delta-%s-%s-alpha-%s\"%(args.distance,args.loss,args.mask_prob,args.tau_low,args.tau_high,args.alpha)\n", "    elif args.loss == 'margin_anchor':\n", "        model_name = \"roberta-%s-%s-mask-%s-delta-%s-%s-alpha-%s\"%(args.distance,args.loss,args.mask_prob,args.tau_low,args.tau_high,args.alpha)\n", "        \n", "    if args.spherical:\n", "        model_name += \"-sp-eta-%s\"%(args.eta)\n", "    if args.attpool != True:\n", "        model_name = 'vallina-' + model_name\n", "        \n", "    if args.prefix:\n", "        model_name = args.prefix + model_name\n", "    \n", "    if not os.path.exists(os.path.join(args.save_model,model_name)):\n", "        os.mkdir(os.path.join(args.save_model,model_name))\n", "        \n", "    if args.train:\n", "        # load data\n", "        if args.save_cache:\n", "            text_tr, label_tr,_,_ = loads(args.training_data)\n", "            text_te, label_te,_,_ = loads(args.develop_data)\n", "            \n", "            text_tr, label_tr = shuffle(text_tr, label_tr)\n", "            text_te, label_te = shuffle(text_te, label_te)\n", "        \n", "            # preprocess the data\n", "            trainA, trainB = preprocess(text_tr)\n", "            testA, testB = preprocess(text_te)\n", "            \n", "            # tokenize the data\n", "            train_a, train_b, tr_attn_a, tr_attn_b = tokenize(trainA, trainB)\n", "            test_a, test_b, te_attn_a, te_attn_b = tokenize(testA, testB)\n", "            \n", "            torch.save((train_a, train_b, tr_attn_a, tr_attn_b,label_tr),os.path.join(args.save_cache,'train'))\n", "            torch.save((test_a, test_b, te_attn_a, te_attn_b,label_te),os.path.join(args.save_cache,'dev'))\n", "            \n", "        elif args.load_cache:\n", "            train_a, train_b, tr_attn_a, tr_attn_b, label_tr = torch.load(os.path.join(args.load_cache,'train'))\n", "            test_a, test_b, te_attn_a, te_attn_b, label_te = torch.load(os.path.join(args.load_cache,'dev'))\n", "        for k in range(args.epochs):\n", "            with open(os.path.join(args.save_model,model_name,'results'),'a+') as out:\n", "                train(args,model,train_a,train_b,tr_attn_a, tr_attn_b, label_tr)\n", "                logits,targets = evaluate(args,test_a,test_b,te_attn_a, te_attn_b, label_te)\n", "                accuracy,f1,auc = compute_metric(logits,targets,threshold=threshold)\n", "                torch.save(model,os.path.join(args.save_model,model_name,'model-%s'%k))\n", "                out.write(\"Epoach:%s-Acc:%s-F1:%s-AUC:%s\\n\"%(k,accuracy,f1,auc))\n", "            \n", "    \n", "    if args.test:\n", "        # load data\n", "        text_te, label_te, users, products = loads(args.test_data)\n", "#        text_te, label_te = shuffle(text_te, label_te)\n", "        if args.save_cache:\n", "            testA, testB = preprocess(text_te)\n", "            test_a, test_b, te_attn_a, te_attn_b = tokenize(testA, testB)\n", "            torch.save((test_a, test_b, te_attn_a, te_attn_b),os.path.join(args.save_cache,'test'))\n", "        elif args.load_cache:\n", "            test_a, test_b, te_attn_a, te_attn_b = torch.load(os.path.join(args.load_cache,'test'))\n", "        \n", "        logits,targets = evaluate(args,test_a,test_b,te_attn_a, te_attn_b, label_te)\n", "        accuracy,f1,auc = compute_metric(logits,targets,threshold=threshold)\n", "        with open(os.path.join(args.save_model,model_name,'results'),'a+') as out:\n", "            out.write('Test-%s\\n'%(args.test_data))\n", "            out.write(\"Acc:%s-F1:%s-AUC:%s\\n\"%(accuracy,f1,auc))\n", "        if args.save_test_data:\n", "            with open(os.path.join(args.save_model,model_name,'full_evaluation'),'w') as out:\n", "                for u, p, logit, target in zip(users,products,logits,targets):\n", "                    out.write(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\n\"%(u,p,logit,target))                                      \n", "    \n", "    \n", "    \n", "      \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    \n", "    "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}